name: ceph_osdi06

ships:
  node0: {ip: &n0ip 192.168.141.101} # NoSSD
  node1: {ip: 192.168.141.107}
  node2: {ip: 192.168.141.108}
  node3: {ip: 192.168.141.132}
  node4: {ip: 192.168.141.147}
  node5: {ip: 192.168.141.144}

ship_defaults:
  docker_port: 2375

  # this is currently ignored, but we're putting it here for later reference and
  # to exemplify how we would like this to be given. This is taken from
  # cloud-config's cloud-init syntax for disk formatting and configuration modules
  # {
  disk_setup:
    /dev/sdb1:
      table_type: 'mbr'
      layout: True
      overwrite: True
    /dev/sde1:
      table_type: 'mbr'
      layout: True
      overwrite: True

  fs_setup:
    - label:  None
      filesystem: 'ext4'
      device: '/dev/sdb1'
      partition: 'auto'

    - label:  None
      filesystem: 'ext4'
      device: '/dev/sdc1'
      partition: 'auto'

    - label:  None
      filesystem: 'ext4'
      device: '/dev/sdd1'
      partition: 'auto'

    - label: None
      filesystem: 'xfs'
      device: '/dev/sde1'
      partition: 'auto'

    - label: None
      filesystem: 'xfs'
      device: '/dev/sde2'
      partition: 'auto'

    - label: None
      filesystem: 'xfs'
      device: '/dev/sde3'
      partition: 'auto'

  mounts:
    - [ sdb1, /mnt/hdd1, "auto" ]
    - [ sdc1, /mnt/hdd2, "auto" ]
    - [ sdd1, /mnt/hdd3, "auto" ]
    - [ sde1, /mnt/ssd1, "auto", "rw,noexec,nodev,noatime,nodiratime,nobarrier", "0", "0" ]
    - [ sde2, /mnt/ssd2, "auto", "rw,noexec,nodev,noatime,nodiratime,nobarrier", "0", "0" ]
    - [ sde3, /mnt/ssd3, "auto", "rw,noexec,nodev,noatime,nodiratime,nobarrier", "0", "0" ]

  mount_default_fields: [ None, None, "auto", "defaults,nobootwait", "0", "2" ]
  # }

service_variables:
  ceph-conf-path: &cephconf /user/ivo/cephconf
  hdd1: &data1 /mnt/hdd1/ceph/
  hdd2: &data2 /mnt/hdd2/ceph/
  hdd3: &data3 /mnt/hdd3/ceph/
  ssd1: &journal1 /mnt/ssd1/ceph/
  ssd2: &journal2 /mnt/ssd2/ceph/
  ssd3: &journal3 /mnt/ssd3/ceph/

services:
  ceph-mon:
    image: ivotron/ceph-mon:0.87.1
    instances:
      ceph-mon:
        ship: node0
        env:
          MON_IP: *n0ip
          MON_NAME: mymon
        volumes:
          /etc/ceph: *cephconf
        net: host
  ceph-osd:
    image: ivotron/ceph-osd:0.87.1
    requires: [ceph-mon]
    env:
      JOURNAL: /var/lib/ceph_journal/journal
    instances:
      ceph-osd-1:
        ship: node1
        env:
          OSD_ID: 0
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data1
          /var/lib/ceph_journal: *journal1
        net: host
      ceph-osd-2:
        ship: node2
        env:
          OSD_ID: 1
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data1
          /var/lib/ceph_journal: *journal1
        net: host
      ceph-osd-3:
        ship: node3
        env:
          OSD_ID: 2
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data1
          /var/lib/ceph_journal: *journal1
        net: host
      ceph-osd-4:
        ship: node4
        env:
          OSD_ID: 3
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data1
          /var/lib/ceph_journal: *journal1
        net: host
      ceph-osd-5:
        ship: node5
        env:
          OSD_ID: 4
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data1
          /var/lib/ceph_journal: *journal1
        net: host
      ceph-osd-6:
        ship: node1
        env:
          OSD_ID: 5
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data2
          /var/lib/ceph_journal: *journal2
        net: host
      ceph-osd-7:
        ship: node2
        env:
          OSD_ID: 6
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data2
          /var/lib/ceph_journal: *journal2
        net: host
      ceph-osd-8:
        ship: node3
        env:
          OSD_ID: 7
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data2
          /var/lib/ceph_journal: *journal2
        net: host
      ceph-osd-9:
        ship: node4
        env:
          OSD_ID: 8
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data2
          /var/lib/ceph_journal: *journal2
        net: host
      ceph-osd-10:
        ship: node5
        env:
          OSD_ID: 9
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data2
          /var/lib/ceph_journal: *journal2
        net: host
      ceph-osd-11:
        ship: node1
        env:
          OSD_ID: 10
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data3
          /var/lib/ceph_journal: *journal3
        net: host
      ceph-osd-12:
        ship: node2
        env:
          OSD_ID: 11
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data3
          /var/lib/ceph_journal: *journal3
        net: host
      ceph-osd-13:
        ship: node3
        env:
          OSD_ID: 12
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data3
          /var/lib/ceph_journal: *journal3
        net: host
      ceph-osd-14:
        ship: node4
        env:
          OSD_ID: 13
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data3
          /var/lib/ceph_journal: *journal3
        net: host
      ceph-osd-15:
        ship: node5
        env:
          OSD_ID: 14
        volumes:
          /etc/ceph: *cephconf
          /var/lib/ceph: *data3
          /var/lib/ceph_journal: *journal3
        net: host
  ceph-osd-cleanup:
    image: stackbrew/ubuntu:trusty
    instances:
      ceph-osd-cleanup-1:
        ship: node1
        command: /bin/sh -c "rm -r /data1/* /data2/* /data3/*"
        volumes:
          /data1: *data1
          /data2: *data2
          /data3: *data3
      ceph-osd-cleanup-2:
        ship: node2
        command: /bin/sh -c "rm -r /data1/* /data2/* /data3/*"
        volumes:
          /data1: *data1
          /data2: *data2
          /data3: *data3
      ceph-osd-cleanup-3:
        ship: node3
        command: /bin/sh -c "rm -r /data1/* /data2/* /data3/*"
        volumes:
          /data1: *data1
          /data2: *data2
          /data3: *data3
      ceph-osd-cleanup-4:
        ship: node4
        command: /bin/sh -c "rm -r /data1/* /data2/* /data3/*"
        volumes:
          /data1: *data1
          /data2: *data2
          /data3: *data3
      ceph-osd-cleanup-5:
        ship: node5
        command: /bin/sh -c "rm -r /data1/* /data2/* /data3/*"
        volumes:
          /data1: *data1
          /data2: *data2
          /data3: *data3
